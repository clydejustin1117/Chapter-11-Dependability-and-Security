<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    <h1>11.3 Safety</h1>
    <article>Safety-critical system where it is essential that system operation is always safe; that is, the system should never damage people or the system's environment even if the system fails. Examples of safety-critical systems include control and monitoring systems in  aircraft , process control system in chemical and pharmaceutical plants, and automobile control systems.</article>
       <article>Hardware control of safety-critical systems is simpler to implement and analyze than software control. However, we now build systems of such complexity that they cannot be controlled by hardware alone. Software control is essential because of the need to manage large numbers of sensors and actuators with complex control laws. For example, advanced, aerodynamically unstable, military aircraft require continual software-controlled adjustment of their flight surfaces to ensure that they do not crash.</article>
       <article>Safety-critical software falls into two classes:</article>
       <p><ol>1. Primary safety software This is software that is embedded as a controller in a system. Malfunctioning of such software can cause a hardware malfunction, which results in human injury or environmental damage. The insulin pump software, introduced in Chapter 1, is an example of a primary safety-critical system. System failure may lead to user injury.</ol></p>
       <p><ol>2. Secondary safety-critical software This is software that can indirectly result in an injury. An example of such software is a computer-aided engineering design system whose malfunctioning might result in a design fault in the object being designed. The fault may cause injury to people if the designed system malfunctions. Another example of a secondary safety-critical system is the mental health care management system, MHC-PMS. Failure of this system, whereby7 an unstable patient may not be treated properly, coulkd lead to that patient injuring themselves or others.</ol></p>
       <p>System reliabilityand system safety are related but a reliable system can be unsafe and vice versa. The software may still behave in such a way that the resultant system behavior leads to an accident. There are four reasons why software systems that are reliable are not necessareily safe:</p>
       <p><ol>1. We can never be 100% certain that a software system is fault-free and fault-tolerant. Undetected faults can be dormant for a long time and software failures can occur after many years of reliable operation.</ol></p>
       <p><ol>2. The specification may be incomplete in that it does not describe the requiredbehavior of the system in some critical situations. A high percentage of systemmalfunctions (Boehm et al., 1975; Endres, 1975; Lutz, 1993; Nakajo and Kume,1991) are the result of specification rather than design errors. In a study of errorsin embedded systems, Lutz concludes:</ol></p>
       <p>...difficulties with requirements are the key root cause of the safetyrelated software errors, which have persisted until integration and system testing.</p>
       <p><ol>3.Hardware malfunctions may cause the system to behave in an unpredictableway, and present the software with an unanticipated environment. When components are close to physical failure, they may behave erratically and generatesignals that are outside the ranges that can be handled by the software.</ol></p>
       <center><img src="d:\User Files\Pictures\ss.png" alt=""></center>
       <section><b>Figure 11.6</b> </section>
       <section>Safety terminology</section>
       <p><ol>4. The system operators may generate inputs that are not individually incorrect butwhich, in some situations, can lead to a system malfunction. An anecdotalexample of this occurred when an aircraft undercarriage collapsed whilst theaircraft was on the ground. Apparently, a technician pressed a button thatinstructed the utility management software to raise the undercarriage. The software carried out the mechanic’s instruction perfectly. However, the systemshould have disallowed the command unless the plane was in the air.</ol></p>
       <p><article>A specialized vocabulary has evolved to discuss safety-critical systems and it isimportant to understand the specific terms used. Figure 11.6 summarizes some definitions of important terms, with examples taken from the insulin pump system.</article></p>

       <p><article>The key to assuring safety is to ensure either that accidents do not occur or thatthe consequences of an accident are minimal. This can be achieved in three complementary ways:</article></p>
       <p><ol>1. Hazard avoidance The system is designed so that hazards are avoided. For example, a cutting system that requires an operator to use two hands to press separate buttons simultaneously avoids the hazard of the operator’s hands beingin the blade pathway</ol></p>
       <p><ol>2. Hazard detection and removal The system is designed so that hazards aredetected and removed before they result in an accident. For example, a chemicalplant system may detect excessive pressure and open a relief valve to reducethese pressures before an explosion occurs.</ol></p>
       <p><ol>3.Damage limitation The system may include protection features that minimizethe damage that may result from an accident. For example, an aircraft enginenormally includes automatic fire extinguishers. If a fire occurs, it can often becontrolled before it poses a threat to the aircraft.</ol></p>
       <article>Accidents most often occur when several things go wrong at the same time. Ananalysis of serious accidents (Perrow, 1984) suggests that they were almost all due toa combination of failures in different parts of a system. Unanticipated combinationsof subsystem failures led to interactions that resulted in overall system failure. Forexample, failure of an air-conditioning system could lead to overheating, which thencauses the system hardware to generate incorrect signals. Perrow also suggests that itis impossible to anticipate all possible combinations of failures. Accidents are therefore an inevitable part of using complex systems.</article>
       <article>Some people have used this as an argument against software control. Because ofthe complexity of software, there are more interactions between the different parts ofa system. This means that there will probably be more combinations of faults thatcould lead to system failure.</article>
       <article>However, software-controlled systems can monitor a wider range of conditionsthan electro-mechanical systems. They can be adapted relatively easily. They usecomputer hardware, which has very high inherent reliability and which is physicallysmall and lightweight. Software-controlled systems can provide sophisticated safetyinterlocks. They can support control strategies that reduce the amount of time peopleneed to spend in hazardous environments. Although software control may introducemore ways in which a system can go wrong, it also allows better monitoring and protection and hence can contribute to improvements in system safety.</article>
       <article>In all cases, it is important to maintain a sense of proportion about system safety.It is impossible to make a system 100% safe and society has to decide whether or notthe consequences of an occasional accident are worth the benefits that come from theuse of advanced technologies. It is also a social and political decision about how todeploy limited national resources to reduce risk to the population as a whole.</article>
    </body>
</html>